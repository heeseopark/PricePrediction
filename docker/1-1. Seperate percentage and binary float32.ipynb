{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters and Lstm hyperparameters\n",
    "window_size = 100 # lstm input size\n",
    "\n",
    "input_window_size = 100\n",
    "\n",
    "target_window_size = 10 # lstm output size\n",
    "\n",
    "hidden_size = 1000\n",
    "\n",
    "num_layers = 4\n",
    "\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDatasetInput(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/PricePrediction/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        tensor = torch.tensor(df.values, dtype=torch.float)  # Return open and close prices\n",
    "        if tensor.size(0) > input_window_size:  # If the tensor is long enough\n",
    "            tensor = tensor[:-input_window_size]  # Remove the last window_size elements\n",
    "        return tensor\n",
    "\n",
    "class PriceDatasetTarget(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/PricePrediction/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        tensor = torch.tensor(df.values, dtype=torch.float)  # Return open and close prices\n",
    "        if tensor.size(0) > input_window_size:  # If the tensor is long enough\n",
    "            tensor = tensor[input_window_size:]  # Remove the last window_size elements\n",
    "        return tensor\n",
    "\n",
    "\n",
    "\n",
    "def sliding_window_percentage_input(batch):\n",
    "    windows_percentage = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - input_window_size + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+input_window_size]\n",
    "            pct_change = ((window[:, 1] - window[:, 0]) * 100 / window[:, 0])\n",
    "            windows_percentage.append(pct_change)\n",
    "    output_percentage = torch.stack(windows_percentage)\n",
    "\n",
    "    return output_percentage\n",
    "\n",
    "def sliding_window_binary_input(batch):\n",
    "    windows_binary = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - input_window_size + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+input_window_size]\n",
    "            binary_change = (window[:, 1] > window[:, 0]).float()  # Calculate the binary change\n",
    "            windows_binary.append(binary_change)\n",
    "    output_binary = torch.stack(windows_binary)\n",
    "\n",
    "    return output_binary\n",
    "\n",
    "\n",
    "def sliding_window_percentage_target(batch):\n",
    "    windows_percentage = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - target_window_size + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+target_window_size]\n",
    "            pct_change = ((window[:, 1] - window[:, 0]) * 100 / window[:, 0])\n",
    "            windows_percentage.append(pct_change)\n",
    "    output_percentage = torch.stack(windows_percentage)\n",
    "\n",
    "    return output_percentage\n",
    "\n",
    "def sliding_window_binary_target(batch):\n",
    "    windows_binary = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - target_window_size + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+target_window_size]\n",
    "            binary_change = (window[:, 1] > window[:, 0]).float()  # Calculate the binary change\n",
    "            windows_binary.append(binary_change)\n",
    "    output_binary = torch.stack(windows_binary)\n",
    "\n",
    "    return output_binary\n",
    "\n",
    "\n",
    "# 인덱스 맞춰봐야함"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_input = PriceDatasetInput('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_dataset_input = PriceDatasetInput('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "\n",
    "\n",
    "percentage_train_loader_input = DataLoader(train_dataset_input, batch_size=1, collate_fn=sliding_window_percentage_input, shuffle=False, drop_last=True)\n",
    "percentage_test_loader_input = DataLoader(test_dataset_input, batch_size=1, collate_fn=sliding_window_percentage_input, shuffle=False, drop_last=True)\n",
    "\n",
    "binary_train_loader_input = DataLoader(train_dataset_input, batch_size=1, collate_fn=sliding_window_binary_input, shuffle=False, drop_last=True)\n",
    "binary_test_loader_input = DataLoader(test_dataset_input, batch_size=1, collate_fn=sliding_window_binary_input, shuffle=False, drop_last=True)\n",
    "\n",
    "\n",
    "\n",
    "train_dataset_target = PriceDatasetTarget('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_dataset_target = PriceDatasetTarget('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "\n",
    "\n",
    "percentage_train_loader_target = DataLoader(train_dataset_target, batch_size=1, collate_fn=sliding_window_percentage_target, shuffle=False, drop_last=True)\n",
    "percentage_test_loader_target = DataLoader(test_dataset_target, batch_size=1, collate_fn=sliding_window_percentage_target, shuffle=False, drop_last=True)\n",
    "\n",
    "binary_train_loader_target = DataLoader(train_dataset_target, batch_size=1, collate_fn=sliding_window_binary_target, shuffle=False, drop_last=True)\n",
    "binary_test_loader_target = DataLoader(test_dataset_target, batch_size=1, collate_fn=sliding_window_binary_target, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercentagePrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PercentagePrediction, self).__init__()\n",
    "        self.lstm_pct = nn.LSTM(input_size = input_window_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.fc_pct = nn.Linear(in_features = hidden_size, out_features = target_window_size, dtype=torch.float)  # output layer for percentage prediction\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm_pct(x, hidden)\n",
    "        out_pct = self.fc_pct(out)  # output for percentage prediction\n",
    "        return out_pct, hidden\n",
    "\n",
    "class BinaryPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryPrediction, self).__init__()\n",
    "        self.lstm_pct = nn.LSTM(input_size = input_window_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.fc_pct = nn.Linear(in_features = hidden_size, out_features = target_window_size, dtype=torch.float)  # output layer for binary prediction\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm_binary(x, hidden)\n",
    "        out_binary = self.fc_binary(out)  # output for binary prediction\n",
    "        return out_binary, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader_input, train_loader_target, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = min(len(train_loader_input), len(train_loader_target))\n",
    "    loss_sum = 0\n",
    "    \n",
    "    # Initialize hidden state\n",
    "    h0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    hidden = (h0, c0)\n",
    "\n",
    "    for i, (batch_input, batch_target) in enumerate(zip(train_loader_input, train_loader_target)):\n",
    "        inputs = batch_input.to(device)\n",
    "        targets = batch_target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)  # Pass the hidden state to the model\n",
    "        hidden = (hidden[0].detach(), hidden[1].detach())  # Detach the hidden state from its history\n",
    "        outputs = outputs.squeeze()  # Remove the extra dimension from outputs\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            avg_loss = loss_sum / (i+1)\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches], Avg Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return loss_sum / total_batches\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(test_loader)  # Total number of batches\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "                print(f\"Testing progress: [{i + 1}/{total_batches} Batches]\")\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1331, 10])) that is different to the input size (torch.Size([1241, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (1241) must match the size of tensor b (1331) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m2\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m---> 10\u001b[0m     train(model \u001b[39m=\u001b[39;49m model, train_loader_input\u001b[39m=\u001b[39;49mpercentage_train_loader_input, train_loader_target\u001b[39m=\u001b[39;49mpercentage_train_loader_target, criterion\u001b[39m=\u001b[39;49mcriterion, optimizer\u001b[39m=\u001b[39;49moptimizer, device \u001b[39m=\u001b[39;49m device)\n",
      "Cell \u001b[1;32mIn[6], line 20\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader_input, train_loader_target, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     17\u001b[0m hidden \u001b[39m=\u001b[39m (hidden[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdetach(), hidden[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mdetach())  \u001b[39m# Detach the hidden state from its history\u001b[39;00m\n\u001b[0;32m     18\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39msqueeze()  \u001b[39m# Remove the extra dimension from outputs\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     21\u001b[0m loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     23\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:536\u001b[0m, in \u001b[0;36mMSELoss.forward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    535\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 536\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mmse_loss(\u001b[39minput\u001b[39;49m, target, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\functional.py:3294\u001b[0m, in \u001b[0;36mmse_loss\u001b[1;34m(input, target, size_average, reduce, reduction)\u001b[0m\n\u001b[0;32m   3291\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   3292\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[1;32m-> 3294\u001b[0m expanded_input, expanded_target \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mbroadcast_tensors(\u001b[39minput\u001b[39;49m, target)\n\u001b[0;32m   3295\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39m_C\u001b[39m.\u001b[39m_nn\u001b[39m.\u001b[39mmse_loss(expanded_input, expanded_target, _Reduction\u001b[39m.\u001b[39mget_enum(reduction))\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\functional.py:74\u001b[0m, in \u001b[0;36mbroadcast_tensors\u001b[1;34m(*tensors)\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function(tensors):\n\u001b[0;32m     73\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(broadcast_tensors, tensors, \u001b[39m*\u001b[39mtensors)\n\u001b[1;32m---> 74\u001b[0m \u001b[39mreturn\u001b[39;00m _VF\u001b[39m.\u001b[39;49mbroadcast_tensors(tensors)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (1241) must match the size of tensor b (1331) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "model = PercentagePrediction().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "epochs = 2\n",
    "\n",
    "for i in range(epochs):\n",
    "    train(model = model, train_loader_input=percentage_train_loader_input, train_loader_target=percentage_train_loader_target, criterion=criterion, optimizer=optimizer, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PriceChangePrediction().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = CustomCriterion().to(device)\n",
    "\n",
    "try: \n",
    "    # Load the saved models and optimizers\n",
    "    checkpoint = torch.load('C:/Github/PricePrediction/docker/models/combined_model.pth')\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "    hidden = (torch.zeros(num_layers, hidden_size).to(device), torch.zeros(num_layers, hidden_size).to(device))  # need to write code for initializing hidden state tensor\n",
    "\n",
    "epochs = 1  # or any other number you prefer\n",
    "\n",
    "train_and_evaluate(model, 'combined_model', train_loader, test_loader, criterion, optimizer, epochs, device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Printing Tensors -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the tensor output by train_loader: 1241\n",
      "tensor([0., 1., 1., 0., 1., 1., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 1., 1.,\n",
      "        0., 1., 1., 0., 0., 1., 1., 0., 1., 1., 0., 1., 1., 1., 1., 0., 0., 0.,\n",
      "        1., 1., 1., 0., 1., 1., 0., 1., 1., 0., 0., 1., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 1., 0., 1., 1., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 1., 0., 1., 0., 1., 0., 1.,\n",
      "        0., 1., 1., 1., 1., 1., 0., 0., 0., 0.])\n",
      "Shape of the tensor output by train_loader: torch.float32\n",
      "tensor([0., 1., 1., 0., 1., 1., 0., 1., 0., 1.])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "# Size of the Dataset\n",
    "print(f'Train dataset size: {train_dataset.__getitem__(1)}')\n",
    "print(f'Test dataset size: {len(train_dataset)}')\n",
    "\n",
    "# Size of the DataLoader (i.e., number of batches)\n",
    "print(f'Train dataloader size: {len(binary_train_loader)}')\n",
    "print(f'Test dataloader size: {len(binary_test_loader)}')\n",
    "\n",
    "# Size of the tensor output by the Dataset\n",
    "sample_tensor = train_dataset[0]\n",
    "print(f'Shape of the tensor output by train_dataset: {sample_tensor.dtype}')\n",
    "\"\"\"\n",
    "# Size of the tensor output by the DataLoader\n",
    "for batch in binary_train_loader_input:\n",
    "    print(f'Shape of the tensor output by train_loader: {batch.size(0)}')\n",
    "    print(batch[103, :])\n",
    "    break  # we break after the first batch\n",
    "\n",
    "\n",
    "for batch in binary_train_loader_target:\n",
    "    print(f'Shape of the tensor output by train_loader: {batch.dtype}')\n",
    "    print(batch[3, :])\n",
    "    break  # we break after the first batch\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Old Codes ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, feature):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        for j in range(batch.shape[0]): # iterate through the first dimension\n",
    "            inputs = batch[j, :, feature].unsqueeze(0).to(device) # add an extra dimension to match the model's expected input shape\n",
    "            percentage_targets = batch[j, 1:, 0].reshape(-1) # Get the percentage change targets\n",
    "            binary_targets = batch[j, 1:, 1].reshape(-1) # Get the binary change targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            percentage_outputs, binary_outputs = model(inputs)\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    for i, batch in enumerate(train_loader):  # Use enumerate to get the index (i)\n",
    "        for j in range(batch.size(0)):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[j, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    for i, batch in enumerate(train_loader):  # Use enumerate to get the index (i)\n",
    "        for j in range(batch.size(0)):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[j, :]\n",
    "            targets = batch[j+window_size, :]  # Get the targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(test_loader)  # Total number of batches\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "                print(f\"Testing progress: [{i + 1}/{total_batches} Batches]\")\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCriterion(nn.Module):\n",
    "    def __init__(self, weights=(1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        self.loss_fn_pct = nn.MSELoss()\n",
    "        self.loss_fn_binary = nn.BCEWithLogitsLoss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, percentage_outputs, binary_outputs, percentage_targets, binary_targets):\n",
    "        loss_pct = self.loss_fn_pct(percentage_outputs, percentage_targets)\n",
    "        loss_binary = self.loss_fn_binary(binary_outputs, binary_targets)\n",
    "        return self.weights[0] * loss_pct + self.weights[1] * loss_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79]\n"
     ]
    }
   ],
   "source": [
    "test = []\n",
    "for i in range(100):\n",
    "    test.append(i)\n",
    "\n",
    "print(test)\n",
    "print(test[80:])\n",
    "print(test[:80])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
