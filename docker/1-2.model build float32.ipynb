{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader, Subset\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pandas import DataFrame as df\n",
    "import mplfinance as mpf\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/PricePrediction/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        return torch.tensor(df.values, dtype=torch.float32)  # Return open and close prices\n",
    "\n",
    "def sliding_window_pct(batch):\n",
    "    windows = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - 100 + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+100]\n",
    "            pct_change = (window[:, 1] - window[:, 0]) * 100 / window[:, 0]\n",
    "            windows.append(pct_change)\n",
    "    return torch.stack(windows)\n",
    "\n",
    "def sliding_window_binary(batch):\n",
    "    windows = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - 100 + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+100]\n",
    "            binary_change = (window[:, 1] > window[:, 0]).float()  # Calculate the binary change\n",
    "            windows.append(binary_change)\n",
    "    return torch.stack(windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "train_dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "train_loader_pct = DataLoader(train_dataset, batch_size=1, collate_fn=sliding_window_pct, shuffle=False, drop_last=True)\n",
    "train_loader_binary = DataLoader(train_dataset, batch_size=1, collate_fn=sliding_window_binary, shuffle=False, drop_last=True)\n",
    "\n",
    "test_dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_loader_pct = DataLoader(test_dataset, batch_size=1, collate_fn=sliding_window_pct, shuffle=False, drop_last=True)\n",
    "test_loader_binary = DataLoader(test_dataset, batch_size=1, collate_fn=sliding_window_binary, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 791\n",
      "Test dataset size: 791\n",
      "Train dataloader size: 791\n",
      "Test dataloader size: 791\n",
      "Shape of the tensor output by train_dataset: torch.float32\n",
      "Shape of the tensor output by train_loader: torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Size of the Dataset\n",
    "print(f'Train dataset size: {len(train_dataset)}')\n",
    "print(f'Test dataset size: {len(test_dataset)}')\n",
    "\n",
    "# Size of the DataLoader (i.e., number of batches)\n",
    "print(f'Train dataloader size: {len(train_loader_binary)}')\n",
    "print(f'Test dataloader size: {len(test_loader_binary)}')\n",
    "\n",
    "# Size of the tensor output by the Dataset\n",
    "sample_tensor = train_dataset[0]\n",
    "print(f'Shape of the tensor output by train_dataset: {sample_tensor.dtype}')\n",
    "\n",
    "# Size of the tensor output by the DataLoader\n",
    "for batch in train_loader_binary:\n",
    "    print(f'Shape of the tensor output by train_loader: {batch.dtype}')\n",
    "    break  # we break after the first batch\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceChangePrediction(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=100, output_dim=1, num_layers=4):\n",
    "        super(PriceChangePrediction, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout = 0.1)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "        # Initialize the hidden state\n",
    "        self.hidden = (torch.zeros(num_layers, 1, hidden_dim),\n",
    "                       torch.zeros(num_layers, 1, hidden_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        hidden = (torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device),\n",
    "                torch.zeros(self.lstm.num_layers, batch_size, self.lstm.hidden_size).to(x.device))\n",
    "        out, _ = self.lstm(x, hidden)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Tensor Shape: torch.Size([1, 100, 1])\n",
      "Input Tensor Type: torch.float32\n",
      "Output Tensor Shape: torch.Size([1, 100, 1])\n",
      "Output Tensor Type: torch.float32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model\n",
    "model = PriceChangePrediction()\n",
    "\n",
    "# Create a sample input tensor of length 100\n",
    "# Assume we are working with batch size 1 and each data point is 1-dimensional\n",
    "input_tensor = torch.randn(1, 100, 1)  # Create a half precision tensor\n",
    "\n",
    "print(f\"Input Tensor Shape: {input_tensor.shape}\")\n",
    "print(f\"Input Tensor Type: {input_tensor.dtype}\")\n",
    "\n",
    "# Feed the input tensor through the model\n",
    "output_tensor = model(input_tensor)\n",
    "\n",
    "print(f\"Output Tensor Shape: {output_tensor.shape}\")\n",
    "print(f\"Output Tensor Type: {output_tensor.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    for i, batch in enumerate(train_loader):  # Use enumerate to get the index (i)\n",
    "        batch = batch.unsqueeze(-1).to(device)\n",
    "        inputs = batch[:, :-1, :]\n",
    "        targets = batch[:, 1:, :]\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i + 1) % 200 == 0:  # Print after every 10 batches\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")\n",
    "\n",
    "\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.unsqueeze(-1).to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            targets = batch[:, 1:, :]\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 1/30 \t percentage_model \t Validation Loss: 0.0161245625\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 2/30 \t percentage_model \t Validation Loss: 0.0161224665\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 3/30 \t percentage_model \t Validation Loss: 0.0161284040\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 4/30 \t percentage_model \t Validation Loss: 0.0161339310\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 5/30 \t percentage_model \t Validation Loss: 0.0161463633\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 6/30 \t percentage_model \t Validation Loss: 0.0161304398\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 7/30 \t percentage_model \t Validation Loss: 0.0161400195\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 8/30 \t percentage_model \t Validation Loss: 0.0161390023\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 9/30 \t percentage_model \t Validation Loss: 0.0161432929\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 10/30 \t percentage_model \t Validation Loss: 0.0161450103\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 11/30 \t percentage_model \t Validation Loss: 0.0161449569\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 12/30 \t percentage_model \t Validation Loss: 0.0161448122\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 13/30 \t percentage_model \t Validation Loss: 0.0161448435\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 14/30 \t percentage_model \t Validation Loss: 0.0161360862\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 15/30 \t percentage_model \t Validation Loss: 0.0161264623\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 16/30 \t percentage_model \t Validation Loss: 0.0161275177\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 17/30 \t percentage_model \t Validation Loss: 0.0161280893\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 18/30 \t percentage_model \t Validation Loss: 0.0161281814\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 19/30 \t percentage_model \t Validation Loss: 0.0161281869\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 20/30 \t percentage_model \t Validation Loss: 0.0161281836\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 21/30 \t percentage_model \t Validation Loss: 0.0161281865\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 22/30 \t percentage_model \t Validation Loss: 0.0161281721\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 23/30 \t percentage_model \t Validation Loss: 0.0161281614\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 24/30 \t percentage_model \t Validation Loss: 0.0161281609\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 25/30 \t percentage_model \t Validation Loss: 0.0161281657\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 26/30 \t percentage_model \t Validation Loss: 0.0161281687\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 27/30 \t percentage_model \t Validation Loss: 0.0161281619\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 28/30 \t percentage_model \t Validation Loss: 0.0161281549\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 29/30 \t percentage_model \t Validation Loss: 0.0161281586\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 30/30 \t percentage_model \t Validation Loss: 0.0161281504\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 1/30 \t binary_model \t Validation Loss: 0.6937809047\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 2/30 \t binary_model \t Validation Loss: 0.6935104097\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 3/30 \t binary_model \t Validation Loss: 0.6936768039\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 4/30 \t binary_model \t Validation Loss: 0.6936677764\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 5/30 \t binary_model \t Validation Loss: 0.6936684495\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 6/30 \t binary_model \t Validation Loss: 0.6936668705\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 7/30 \t binary_model \t Validation Loss: 0.6936664640\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 8/30 \t binary_model \t Validation Loss: 0.6936673725\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 9/30 \t binary_model \t Validation Loss: 0.6936666473\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 10/30 \t binary_model \t Validation Loss: 0.6936669842\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 11/30 \t binary_model \t Validation Loss: 0.6936677086\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 12/30 \t binary_model \t Validation Loss: 0.6936671912\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 13/30 \t binary_model \t Validation Loss: 0.6936672204\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 14/30 \t binary_model \t Validation Loss: 0.6936672029\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 15/30 \t binary_model \t Validation Loss: 0.6936665165\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 16/30 \t binary_model \t Validation Loss: 0.6936671058\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 17/30 \t binary_model \t Validation Loss: 0.6936673961\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 18/30 \t binary_model \t Validation Loss: 0.6936673970\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 19/30 \t binary_model \t Validation Loss: 0.6936674905\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 20/30 \t binary_model \t Validation Loss: 0.6936673361\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 21/30 \t binary_model \t Validation Loss: 0.6936674216\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 22/30 \t binary_model \t Validation Loss: 0.6936674465\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 23/30 \t binary_model \t Validation Loss: 0.6936674170\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 24/30 \t binary_model \t Validation Loss: 0.6936674145\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 25/30 \t binary_model \t Validation Loss: 0.6936673985\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 26/30 \t binary_model \t Validation Loss: 0.6936674258\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 27/30 \t binary_model \t Validation Loss: 0.6936669804\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 28/30 \t binary_model \t Validation Loss: 0.6936677202\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 29/30 \t binary_model \t Validation Loss: 0.6936672376\n",
      "Training progress: [200/791 Batches]\n",
      "Training progress: [400/791 Batches]\n",
      "Training progress: [600/791 Batches]\n",
      "Epoch 30/30 \t binary_model \t Validation Loss: 0.6936673822\n"
     ]
    }
   ],
   "source": [
    "# Create the models\n",
    "percentage_model = PriceChangePrediction().to(device)\n",
    "binary_model = PriceChangePrediction().to(device)\n",
    "\n",
    "# Define the loss functions\n",
    "percentage_criterion = nn.MSELoss()\n",
    "binary_criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Define the optimizers\n",
    "percentage_optimizer = torch.optim.Adam(percentage_model.parameters(), lr=0.01)\n",
    "binary_optimizer = torch.optim.Adam(binary_model.parameters(), lr=0.01)\n",
    "\n",
    "try: \n",
    "    # Load the saved models and optimizers\n",
    "    percentage_checkpoint = torch.load('C:/Github/PricePrediction/docker/models/percentage_model.pth')\n",
    "    binary_checkpoint = torch.load('C:/Github/PricePrediction/docker/models/binary_model.pth')\n",
    "\n",
    "    percentage_model.load_state_dict(percentage_checkpoint['model_state_dict'])\n",
    "    binary_model.load_state_dict(binary_checkpoint['model_state_dict'])\n",
    "\n",
    "    percentage_optimizer.load_state_dict(percentage_checkpoint['optimizer_state_dict'])\n",
    "    binary_optimizer.load_state_dict(binary_checkpoint['optimizer_state_dict'])\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Set the number of epochs\n",
    "epochs = 30\n",
    "\n",
    "# Train and evaluate PriceChangePrediction model\n",
    "train_and_evaluate(percentage_model, 'percentage_model', train_loader_pct, test_loader_pct, percentage_criterion, percentage_optimizer, epochs, device=device)\n",
    "\n",
    "# Train and evaluate PriceDirectionPrediction model\n",
    "train_and_evaluate(binary_model, 'binary_model', train_loader_binary, test_loader_binary, binary_criterion, binary_optimizer, epochs, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Model's state_dict:\")\n",
    "for param_tensor in percentage_model.state_dict():\n",
    "    print(param_tensor, \"\\t\", percentage_model.state_dict()[param_tensor].size())\n",
    "\n",
    "print(\"Optimizer's state_dict:\")\n",
    "for var_name in percentage_optimizer.state_dict():\n",
    "    print(var_name, \"\\t\", percentage_optimizer.state_dict()[var_name])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch 1/1 | percentage_model Validation Loss: 0.0161349802\n",
    "Epoch 1/1 | binary_model Validation Loss: 0.6933783825"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_input = torch.randn(1, 100, 1).to(device)  # Create a random tensor of shape (1, 100, 1)\n",
    "random_output = percentage_model(random_input.half())  # Get the output\n",
    "\n",
    "print(random_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
