{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset parameters and Lstm hyperparameters\n",
    "window_size = 100 # lstm input size\n",
    "\n",
    "input_window_size = 100\n",
    "\n",
    "target_window_size = 10 # lstm output size\n",
    "\n",
    "output_size = 1\n",
    "\n",
    "hidden_size = 1000\n",
    "\n",
    "num_layers = 4\n",
    "\n",
    "dropout = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'C:/Github/PricePrediction/csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        tensor = torch.tensor(df.values, dtype=torch.float)  # Return open and close prices\n",
    "        return tensor\n",
    "\n",
    "\n",
    "def sliding_window_percentage(batch):\n",
    "    windows_percentage = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - input_window_size - target_window_size + 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+input_window_size+target_window_size]\n",
    "            pct_change = ((window[:, 1] - window[:, 0]) * 100 / window[:, 0])\n",
    "            windows_percentage.append(pct_change)\n",
    "    output_percentage = torch.stack(windows_percentage)\n",
    "\n",
    "    return output_percentage\n",
    "\n",
    "def sliding_window_binary(batch):\n",
    "    windows_binary = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - input_window_size - target_window_size+ 1):  # Create windows of size window_size\n",
    "            window = tensor[i:i+input_window_size+target_window_size]\n",
    "            binary_change = (window[:, 1] > window[:, 0]).float()  # Calculate the binary change\n",
    "            windows_binary.append(binary_change)\n",
    "    output_binary = torch.stack(windows_binary)\n",
    "\n",
    "    return output_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "\n",
    "\n",
    "percentage_train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sliding_window_percentage, shuffle=False, drop_last=True)\n",
    "percentage_test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sliding_window_percentage, shuffle=False, drop_last=True)\n",
    "\n",
    "binary_train_loader = DataLoader(train_dataset, batch_size=1, collate_fn=sliding_window_binary, shuffle=False, drop_last=True)\n",
    "binary_test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sliding_window_binary, shuffle=False, drop_last=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PercentagePrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(PercentagePrediction, self).__init__()\n",
    "        self.lstm_pct = nn.LSTM(input_size = input_window_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.fc_pct = nn.Linear(in_features = hidden_size, out_features = output_size, dtype=torch.float)  # output layer for percentage prediction\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm_pct(x, hidden)\n",
    "        out_pct = self.fc_pct(out)  # output for percentage prediction\n",
    "        return out_pct, hidden\n",
    "\n",
    "class BinaryPrediction(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(BinaryPrediction, self).__init__()\n",
    "        self.lstm_binary = nn.LSTM(input_size = input_window_size, hidden_size = hidden_size, num_layers = num_layers, dropout = dropout)\n",
    "        self.fc_binary = nn.Linear(in_features = hidden_size, out_features = output_size, dtype=torch.float)  # output layer for binary prediction\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        out, hidden = self.lstm_binary(x, hidden)\n",
    "        out_binary = self.fc_binary(out)  # output for binary prediction\n",
    "        return out_binary, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.to(device).train()\n",
    "    total_batches = len(train_loader)\n",
    "    loss_sum = float(0)\n",
    "\n",
    "    # Initialize hidden state\n",
    "    h0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    hidden = (h0, c0)\n",
    "\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs = batch[:, :input_window_size].to(device)\n",
    "        targets = batch[:, input_window_size:].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs, hidden = model(inputs, hidden)  # Pass the hidden state to the model\n",
    "        hidden = (hidden[0].detach().to(device), hidden[1].detach().to(device))  # Detach the hidden state from its history\n",
    "\n",
    "        outputs = outputs.to(device)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if torch.isnan(torch.tensor(loss_sum)):\n",
    "            print(f\"{i} batch calculated nan\")\n",
    "            print(inputs.detach().cpu())\n",
    "            print(outputs.detach().cpu())\n",
    "\n",
    "            zero_indices = (inputs == 0).nonzero(as_tuple=True)\n",
    "            one_indices = (inputs == 1).nonzero(as_tuple=True)\n",
    "            nan_indices = (inputs == torch.nan).nonzero(as_tuple=True)\n",
    "            \n",
    "            print(\"Input Indices of 0s: \", zero_indices)\n",
    "            print(\"Input Indices of 1s: \", one_indices)\n",
    "            print(\"Input Incides of nans: \", nan_indices)\n",
    "\n",
    "            # Get the indices of elements in outputs that are 0 or 1\n",
    "            zero_indices = (outputs == 0).nonzero(as_tuple=True)\n",
    "            one_indices = (outputs == 1).nonzero(as_tuple=True)\n",
    "            nan_indices = (outputs == torch.nan).nonzero(as_tuple=True)\n",
    "            \n",
    "            print(\"Output Indices of 0s: \", zero_indices)\n",
    "            print(\"Output Indices of 1s: \", one_indices)\n",
    "            print(\"Output Incides of nans: \", nan_indices)\n",
    "\n",
    "            return\n",
    "\n",
    "\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            avg_loss = loss_sum / (i+1)\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches] \\t Avg Loss: {avg_loss:.10f}\")\n",
    "\n",
    "\n",
    "    return loss_sum / total_batches\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(test_loader)  # Total number of batches\n",
    "\n",
    "    # Initialize hidden state\n",
    "    h0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    c0 = torch.zeros(num_layers, hidden_size).to(device)\n",
    "    hidden = (h0, c0)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            inputs = batch[:, :input_window_size].to(device)\n",
    "            targets = batch[:, input_window_size:].to(device)\n",
    "\n",
    "            outputs, _ = model(inputs, hidden)  # Get the two outputs\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "                avg_loss = total_loss / (i+1)\n",
    "                print(f\"Testing progress: [{i + 1}/{total_batches} Batches] \\t Avg Loss: {avg_loss:.10f}\")\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Try to load existing model and optimizer states\n",
    "    try:\n",
    "        # Load the saved models and optimizers\n",
    "        checkpoint = torch.load(f'models/{str(modelname)}.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Checkpoint loaded. Resuming training from there.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model = model, train_loader=train_loader, criterion=criterion, optimizer=optimizer, device=device)\n",
    "        val_loss = evaluate(model=model, test_loader=test_loader, criterion=criterion, device=device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming training from there.\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0163327558\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0126771044\n",
      "Training progress: [600/791 Batches] \t Avg Loss: 0.0115010745\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0278223085\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0201047058\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0187871945\n",
      "Epoch 1/10 \t test model \t Validation Loss: 0.0160391741\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0163277722\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mAdam(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m0.001\u001b[39m)\n\u001b[0;32m      5\u001b[0m criterion \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mMSELoss()\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 7\u001b[0m train_and_evaluate(model \u001b[39m=\u001b[39;49m model, modelname \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mtest model\u001b[39;49m\u001b[39m'\u001b[39;49m, train_loader\u001b[39m=\u001b[39;49mpercentage_train_loader, test_loader\u001b[39m=\u001b[39;49mpercentage_test_loader, criterion\u001b[39m=\u001b[39;49mcriterion, optimizer\u001b[39m=\u001b[39;49moptimizer, device \u001b[39m=\u001b[39;49m device, epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[15], line 100\u001b[0m, in \u001b[0;36mtrain_and_evaluate\u001b[1;34m(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device)\u001b[0m\n\u001b[0;32m     97\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mNo checkpoint found. Starting from scratch.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     99\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(epochs):\n\u001b[1;32m--> 100\u001b[0m     train(model \u001b[39m=\u001b[39;49m model, train_loader\u001b[39m=\u001b[39;49mtrain_loader, criterion\u001b[39m=\u001b[39;49mcriterion, optimizer\u001b[39m=\u001b[39;49moptimizer, device\u001b[39m=\u001b[39;49mdevice)\n\u001b[0;32m    101\u001b[0m     val_loss \u001b[39m=\u001b[39m evaluate(model\u001b[39m=\u001b[39mmodel, test_loader\u001b[39m=\u001b[39mtest_loader, criterion\u001b[39m=\u001b[39mcriterion, device\u001b[39m=\u001b[39mdevice)\n\u001b[0;32m    102\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{\u001b[39;00mepochs\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m{\u001b[39;00mmodelname\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\\t\u001b[39;00m\u001b[39m Validation Loss: \u001b[39m\u001b[39m{\u001b[39;00mval_loss\u001b[39m:\u001b[39;00m\u001b[39m.10f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[15], line 26\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer, device)\u001b[0m\n\u001b[0;32m     23\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, targets)\n\u001b[0;32m     24\u001b[0m loss_sum \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m---> 26\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m     27\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m     29\u001b[0m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39misnan(torch\u001b[39m.\u001b[39mtensor(loss_sum)):\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    489\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\heeseopark\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = PercentagePrediction().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.MSELoss().to(device)\n",
    "\n",
    "train_and_evaluate(model = model, modelname = 'percentage test model', train_loader=percentage_train_loader, test_loader=percentage_test_loader, criterion=criterion, optimizer=optimizer, device = device, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint loaded. Resuming training from there.\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0006504046\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0006994279\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003572118\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0054584569\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0031136059\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0013784325\n",
      "Epoch 1/10 \t binary test model \t Validation Loss: 0.0012619488\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0006932730\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0007282632\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003606795\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0055666533\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0031701315\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0013927884\n",
      "Epoch 2/10 \t binary test model \t Validation Loss: 0.0012751596\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0007078977\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0007470924\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003834187\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0057549551\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0032700787\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0014257628\n",
      "Epoch 3/10 \t binary test model \t Validation Loss: 0.0013046895\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0007419627\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0007805727\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003987763\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0059731534\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0033879883\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0014674538\n",
      "Epoch 4/10 \t binary test model \t Validation Loss: 0.0013425705\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0007994317\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0008204995\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003851599\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0060748508\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0034436266\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0014879902\n",
      "Epoch 5/10 \t binary test model \t Validation Loss: 0.0013613117\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0008256780\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0008493323\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0003969268\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0062680177\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0035495053\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0015274726\n",
      "Epoch 6/10 \t binary test model \t Validation Loss: 0.0013975003\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0008626988\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0008806283\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0004018811\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0064288375\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0036380573\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0015609555\n",
      "Epoch 7/10 \t binary test model \t Validation Loss: 0.0014282986\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0008893496\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0009040116\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0004086416\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0065638446\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0037126046\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0015893487\n",
      "Epoch 8/10 \t binary test model \t Validation Loss: 0.0014544814\n",
      "Training progress: [200/791 Batches] \t Avg Loss: 0.0009081450\n",
      "Training progress: [400/791 Batches] \t Avg Loss: 0.0009214512\n",
      "Training progress: [600/791 Batches] \t Avg Loss: -0.0004167037\n",
      "Testing progress: [200/791 Batches] \t Avg Loss: 0.0066711467\n",
      "Testing progress: [400/791 Batches] \t Avg Loss: 0.0037743033\n",
      "Testing progress: [600/791 Batches] \t Avg Loss: 0.0016171713\n",
      "Epoch 9/10 \t binary test model \t Validation Loss: 0.0014790171\n"
     ]
    }
   ],
   "source": [
    "model = BinaryPrediction().to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "\n",
    "train_and_evaluate(model = model, modelname = 'binary test model', train_loader=percentage_train_loader, test_loader=percentage_test_loader, criterion=criterion, optimizer=optimizer, device = device, epochs=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Printing Tensors -------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# Size of the Dataset\n",
    "print(f'Train dataset size: {train_dataset.__getitem__(1)}')\n",
    "print(f'Test dataset size: {len(train_dataset)}')\n",
    "\n",
    "# Size of the DataLoader (i.e., number of batches)\n",
    "print(f'Train dataloader size: {len(binary_train_loader)}')\n",
    "print(f'Test dataloader size: {len(binary_test_loader)}')\n",
    "\n",
    "# Size of the tensor output by the Dataset\n",
    "sample_tensor = train_dataset[0]\n",
    "print(f'Shape of the tensor output by train_dataset: {sample_tensor.dtype}')\n",
    "\"\"\"\n",
    "# Size of the tensor output by the DataLoader\n",
    "for batch in binary_train_loader:\n",
    "    print(f'Shape of the tensor output by train_loader: {batch.size()}')\n",
    "    print(batch[3, :])\n",
    "    break  # we break after the first batch\n",
    "\n",
    "\n",
    "for batch in binary_train_loader:\n",
    "    print(f'Shape of the tensor output by train_loader: {batch.dtype}')\n",
    "    print(batch[3, :])\n",
    "    break  # we break after the first batch\n",
    "\n",
    "print(len(binary_train_loader))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------ Old Codes ------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device, feature):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)\n",
    "    for i, batch in enumerate(train_loader):\n",
    "        for j in range(batch.shape[0]): # iterate through the first dimension\n",
    "            inputs = batch[j, :, feature].unsqueeze(0).to(device) # add an extra dimension to match the model's expected input shape\n",
    "            percentage_targets = batch[j, 1:, 0].reshape(-1) # Get the percentage change targets\n",
    "            binary_targets = batch[j, 1:, 1].reshape(-1) # Get the binary change targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            percentage_outputs, binary_outputs = model(inputs)\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    for i, batch in enumerate(train_loader):  # Use enumerate to get the index (i)\n",
    "        for j in range(batch.size(0)):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[j, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    # Try to load existing model and optimizer states\n",
    "    try:\n",
    "        # Load the saved models and optimizers\n",
    "        checkpoint = torch.load(f'models/{str(modelname)}.pth')\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "        print(\"Checkpoint loaded. Resuming training from there.\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"No checkpoint found. Starting from scratch.\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_batches = len(train_loader)  # Total number of batches\n",
    "    for i, batch in enumerate(train_loader):  # Use enumerate to get the index (i)\n",
    "        for j in range(batch.size(0)):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[j, :]\n",
    "            targets = batch[j+window_size, :]  # Get the targets\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "            print(f\"Training progress: [{i + 1}/{total_batches} Batches]\")\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_batches = len(test_loader)  # Total number of batches\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(test_loader):\n",
    "            batch = batch.to(device)\n",
    "            inputs = batch[:, :-1, :]\n",
    "            percentage_targets = batch[:, 1:, 0]  # Get the percentage change targets\n",
    "            binary_targets = batch[:, 1:, 1]  # Get the binary change targets\n",
    "\n",
    "            percentage_targets = percentage_targets.reshape(-1)\n",
    "            binary_targets = binary_targets.reshape(-1)\n",
    "\n",
    "            percentage_outputs, binary_outputs = model(inputs)  # Get the two outputs\n",
    "            loss = criterion(percentage_outputs, binary_outputs, percentage_targets, binary_targets)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            if (i + 1) % 200 == 0:  # Print after every 200 batches\n",
    "                print(f\"Testing progress: [{i + 1}/{total_batches} Batches]\")\n",
    "    return total_loss / len(test_loader)  # Return average loss\n",
    "\n",
    "def train_and_evaluate(model, modelname, train_loader, test_loader, criterion, optimizer, epochs, device):\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        train(model, train_loader, criterion, optimizer, device)\n",
    "        val_loss = evaluate(model, test_loader, criterion, device)\n",
    "        print(f\"Epoch {epoch+1}/{epochs} \\t {modelname} \\t Validation Loss: {val_loss:.10f}\")\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if val_loss < best_val_loss:\n",
    "            torch.save({\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "            }, f'models/{str(modelname)}.pth')\n",
    "            best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCriterion(nn.Module):\n",
    "    def __init__(self, weights=(1.0, 1.0)):\n",
    "        super().__init__()\n",
    "        self.loss_fn_pct = nn.MSELoss()\n",
    "        self.loss_fn_binary = nn.BCEWithLogitsLoss()\n",
    "        self.weights = weights\n",
    "\n",
    "    def forward(self, percentage_outputs, binary_outputs, percentage_targets, binary_targets):\n",
    "        loss_pct = self.loss_fn_pct(percentage_outputs, percentage_targets)\n",
    "        loss_binary = self.loss_fn_binary(binary_outputs, binary_targets)\n",
    "        return self.weights[0] * loss_pct + self.weights[1] * loss_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PriceChangePrediction().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = CustomCriterion().to(device)\n",
    "\n",
    "try: \n",
    "    # Load the saved models and optimizers\n",
    "    checkpoint = torch.load('C:/Github/PricePrediction/docker/models/combined_model.pth')\n",
    "\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(\"No checkpoint found. Starting from scratch.\")\n",
    "    hidden = (torch.zeros(num_layers, hidden_size).to(device), torch.zeros(num_layers, hidden_size).to(device))  # need to write code for initializing hidden state tensor\n",
    "\n",
    "epochs = 1  # or any other number you prefer\n",
    "\n",
    "train_and_evaluate(model, 'combined_model', train_loader, test_loader, criterion, optimizer, epochs, device)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
