{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluate ETHUSDT price using model built by BTCUSDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader, Subset\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pandas import DataFrame as df\n",
    "import mplfinance as mpf\n",
    "#import mathplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        df = df.diff(axis=1)[1] > 0  # Compute if close price is higher than open\n",
    "        labels = df.astype(int).values  # Convert to integer labels\n",
    "        return torch.tensor(labels, dtype=torch.float32)\n",
    "\n",
    "# Dataset is supposed to be [number of dates used][number of rows in each csv file][6, means the number of columns we are using in each csv file]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the dataset\n",
    "dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_fn(batch):\n",
    "    windows = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - 100 + 1):  # Create windows of 100 rows each\n",
    "            windows.append(tensor[i:i+100])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "\n",
    "\n",
    "# Shuffle the dataset indices\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split the indices into training and test sets\n",
    "split_idx = int(0.8 * len(indices))\n",
    "train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# Create data subsets using the indices\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=1, collate_fn=sliding_window_fn, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=sliding_window_fn, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch index: 0, Batch tensor shape: torch.Size([1341, 100])\n",
      "Batch index: 0, Batch tensor shape: torch.Size([1261, 100])\n"
     ]
    }
   ],
   "source": [
    "# To view first batch of the training data loader\n",
    "for batch_idx, batch in enumerate(train_loader):\n",
    "    print(f'Batch index: {batch_idx}, Batch tensor shape: {batch.shape}')\n",
    "    break  # To prevent printing all the data\n",
    "\n",
    "# To view first batch of the test data loader\n",
    "for batch_idx, batch in enumerate(test_loader):\n",
    "    print(f'Batch index: {batch_idx}, Batch tensor shape: {batch.shape}')\n",
    "    break  # To prevent printing all the data\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        # Defining the number of layers and the nodes in each layer\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "\n",
    "        # LSTM layers\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "        \n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_().to(device)\n",
    "\n",
    "        out, (hn, cn) = self.lstm(x, (h0.detach(), c0.detach()))\n",
    "\n",
    "        # Only take the output from the final timestep\n",
    "        out = self.fc(out[:, -1, :]) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTM(\n",
       "  (lstm): LSTM(1, 50, batch_first=True)\n",
       "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model, Loss, and Optimizer\n",
    "input_dim = 1   # We are now dealing with 1D sequences\n",
    "hidden_dim = 50   \n",
    "layer_dim = 1\n",
    "output_dim = 2  # Now output will be of size 2 (for 'rise' and 'fall')\n",
    "\n",
    "model = LSTM(input_dim, hidden_dim, layer_dim, output_dim)\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load('binaryclassification.pth')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    }
   ],
   "source": [
    "def predict(model, data_loader):\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for batch in data_loader:\n",
    "            features = batch[:, :-1].unsqueeze(-1).to(device)  # Exclude the last label from features\n",
    "            targets = batch[:, -1].float().to(device)  # Last label is the target\n",
    "\n",
    "            output = model(features)\n",
    "\n",
    "            # Only take the last output for each sequence\n",
    "            last_output = output[:, -1]\n",
    "\n",
    "            # Use a threshold to determine the class\n",
    "            predicted = (last_output.data > 0.5).float()\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "        print(f'Accuracy: {correct / total:.2f}')\n",
    "\n",
    "\n",
    "predict(model, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
