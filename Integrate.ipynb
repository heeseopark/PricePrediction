{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import IterableDataset, DataLoader, Subset\n",
    "from datetime import datetime as dt, timedelta\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "from pandas import DataFrame as df\n",
    "import mplfinance as mpf\n",
    "\n",
    "# check if CUDA is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "seed = 42  # choose any seed you prefer\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "class PriceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, item, timespan, start_date_str, end_date_str):\n",
    "        self.directory = f'csvfiles/{item}'\n",
    "        self.item = item\n",
    "        self.timespan = timespan\n",
    "        start_date = dt.strptime(start_date_str, '%Y-%m-%d').date()\n",
    "        end_date = dt.strptime(end_date_str, '%Y-%m-%d').date()\n",
    "        self.dates = [single_date.strftime(\"%Y-%m-%d\") for single_date in self.daterange(start_date, end_date)]\n",
    "        self.columns = [1, 4]  # Selecting open and close prices\n",
    "        self.filenames = self.get_filenames()\n",
    "\n",
    "    def daterange(self, start_date, end_date):\n",
    "        for n in range(int((end_date - start_date).days) + 1):\n",
    "            yield start_date + timedelta(n)\n",
    "\n",
    "    def get_filenames(self):\n",
    "        filenames = []\n",
    "        for date in self.dates:\n",
    "            filename = f\"{self.directory}/{self.item}-{self.timespan}-{date}.csv\"\n",
    "            if os.path.exists(filename):\n",
    "                filenames.append(filename)\n",
    "        return filenames\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "        df = pd.read_csv(filename, usecols=self.columns, header=None)\n",
    "        df = df[df.columns[::-1]]  # Swap the columns\n",
    "        df = df.diff(axis=1)[1]  # Compute difference between close and open price for each row\n",
    "        return torch.tensor(df.values, dtype=torch.float32)  # Convert to tensor\n",
    "\n",
    "\n",
    "def sliding_window_fn(batch):\n",
    "    windows = []\n",
    "    for tensor in batch:\n",
    "        for i in range(tensor.shape[0] - 100 + 1):  # Create windows of 100 rows each\n",
    "            windows.append(tensor[i:i+100])\n",
    "    return torch.stack(windows)\n",
    "\n",
    "\n",
    "# Create the dataset\n",
    "dataset = PriceDataset('BTCUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "\n",
    "# Shuffle the dataset indices\n",
    "indices = list(range(len(dataset)))\n",
    "random.shuffle(indices)\n",
    "\n",
    "# Split the indices into training and test sets\n",
    "split_idx = int(0.8 * len(indices))\n",
    "train_indices, test_indices = indices[:split_idx], indices[split_idx:]\n",
    "\n",
    "# Create data subsets using the indices\n",
    "train_data = Subset(dataset, train_indices)\n",
    "test_data = Subset(dataset, test_indices)\n",
    "\n",
    "# Create the data loaders\n",
    "train_loader = DataLoader(train_data, batch_size=1, collate_fn=sliding_window_fn, shuffle=False, drop_last=True)\n",
    "test_loader = DataLoader(test_data, batch_size=1, collate_fn=sliding_window_fn, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM1(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=2):\n",
    "        super(LSTM1, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM2(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=3):\n",
    "        super(LSTM2, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM3(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=4):\n",
    "        super(LSTM3, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM4(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=2, dropout_prob=0.05):\n",
    "        super(LSTM4, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM5(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=3, dropout_prob=0.05):\n",
    "        super(LSTM5, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM6(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=50, output_dim=10, num_layers=4, dropout_prob=0.05):\n",
    "        super(LSTM6, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Define the LSTM layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=dropout_prob if num_layers > 1 else 0)\n",
    "\n",
    "        # Define dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "\n",
    "        # Define the output layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape the input tensor to [batch_size, sequence_length, number_of_features]\n",
    "        x = x.view(x.size(0), -1, 1)\n",
    "\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Initialize cell state\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "\n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "\n",
    "        # Dropout\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        # Index hidden state of last time step\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data = data.to(device)  # Move the data to the device (CPU or GPU)\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "        outputs = model(data)  # Forward pass\n",
    "        loss = criterion(outputs, data[:, -10:])  # Compute the loss\n",
    "        loss.backward()  # Backward pass\n",
    "        optimizer.step()  # Update the weights\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)  # Move the data to the device\n",
    "            outputs = model(data)  # Forward pass\n",
    "            loss = criterion(outputs, data[:, -10:])  # Compute the loss\n",
    "            test_loss += loss.item() * data.size(0)  # Accumulate the loss\n",
    "    return test_loss / len(test_loader.dataset)  # Return the average loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1112101.0842317282\n",
      "Epoch 2, Validation Loss: 719459.4456721559\n",
      "Epoch 3, Validation Loss: 538648.3276170747\n",
      "Epoch 4, Validation Loss: 506825.25704077206\n",
      "Epoch 5, Validation Loss: 394801.9010578574\n",
      "Epoch 6, Validation Loss: 353163.5705616834\n",
      "Epoch 7, Validation Loss: 346141.84983545745\n",
      "Epoch 8, Validation Loss: 302189.71886329434\n",
      "Epoch 9, Validation Loss: 324146.00469935016\n",
      "Epoch 10, Validation Loss: 328851.2783269472\n",
      "Epoch 11, Validation Loss: 322268.86401912704\n",
      "Epoch 12, Validation Loss: 264814.7880968605\n",
      "Epoch 13, Validation Loss: 323717.6403576773\n",
      "Epoch 14, Validation Loss: 396496.7335491038\n",
      "Epoch 15, Validation Loss: 273452.96557028225\n",
      "Epoch 16, Validation Loss: 330579.3209189619\n",
      "Epoch 17, Validation Loss: 501321.24927362666\n",
      "Epoch 18, Validation Loss: 550645.8989096085\n",
      "Epoch 19, Validation Loss: 356358.3583924572\n",
      "Epoch 20, Validation Loss: 303201.80416226876\n",
      "Epoch 21, Validation Loss: 295392.65266450436\n",
      "Epoch 22, Validation Loss: 276591.03075267834\n",
      "Epoch 23, Validation Loss: 257184.7780770624\n",
      "Epoch 24, Validation Loss: 299664.3431384668\n",
      "Epoch 25, Validation Loss: 256635.15572342594\n",
      "Epoch 26, Validation Loss: 227213.31567146344\n",
      "Epoch 27, Validation Loss: 265269.98642823455\n",
      "Epoch 28, Validation Loss: 201616.7909415593\n",
      "Epoch 29, Validation Loss: 212601.08140528895\n",
      "Epoch 30, Validation Loss: 219513.89173317555\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model1 = LSTM1().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model1.parameters(), lr=0.01)\n",
    "epochs = 30\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model1, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model1, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "    \n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model1_state_dict': model1.state_dict(),\n",
    "            'optimizer1_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model1.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1068614.7496229103\n",
      "Epoch 2, Validation Loss: 630526.017917182\n",
      "Epoch 3, Validation Loss: 471318.5069773023\n",
      "Epoch 4, Validation Loss: 386285.42429359304\n",
      "Epoch 5, Validation Loss: 342438.95227487874\n",
      "Epoch 6, Validation Loss: 307649.77981084905\n",
      "Epoch 7, Validation Loss: 279437.3632256429\n",
      "Epoch 8, Validation Loss: 262020.06927101934\n",
      "Epoch 9, Validation Loss: 230864.25246383052\n",
      "Epoch 10, Validation Loss: 212732.95617764496\n",
      "Epoch 11, Validation Loss: 272415.0592748785\n",
      "Epoch 12, Validation Loss: 209424.47819671044\n",
      "Epoch 13, Validation Loss: 198904.31725547693\n",
      "Epoch 14, Validation Loss: 238949.11748093105\n",
      "Epoch 15, Validation Loss: 181628.49279202343\n",
      "Epoch 16, Validation Loss: 190122.5059371525\n",
      "Epoch 17, Validation Loss: 178670.6603817624\n",
      "Epoch 18, Validation Loss: 155444.79288817875\n",
      "Epoch 19, Validation Loss: 250591.8812130847\n",
      "Epoch 20, Validation Loss: 187356.46489960985\n",
      "Epoch 21, Validation Loss: 184704.89086306095\n",
      "Epoch 22, Validation Loss: 198908.87932050604\n",
      "Epoch 23, Validation Loss: 189757.6684171522\n",
      "Epoch 24, Validation Loss: 178322.01159322186\n",
      "Epoch 25, Validation Loss: 174688.4104154035\n",
      "Epoch 26, Validation Loss: 230003.44203234956\n",
      "Epoch 27, Validation Loss: 172532.03719271204\n",
      "Epoch 28, Validation Loss: 212462.28757444944\n",
      "Epoch 29, Validation Loss: 198608.55170161207\n",
      "Epoch 30, Validation Loss: 189314.0677487972\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model2 = LSTM2().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model2.parameters(), lr=0.01)\n",
    "epochs = 30\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model2, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model2, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model2_state_dict': model2.state_dict(),\n",
    "            'optimizer2_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model2.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1154072.2291340968\n",
      "Epoch 2, Validation Loss: 691684.178246335\n",
      "Epoch 3, Validation Loss: 475879.87758454983\n",
      "Epoch 4, Validation Loss: 396345.53189665556\n",
      "Epoch 5, Validation Loss: 322840.48409273743\n",
      "Epoch 6, Validation Loss: 293984.614943542\n",
      "Epoch 7, Validation Loss: 258572.74423542302\n",
      "Epoch 8, Validation Loss: 251690.59911739826\n",
      "Epoch 9, Validation Loss: 224378.5199593798\n",
      "Epoch 10, Validation Loss: 213667.5119355469\n",
      "Epoch 11, Validation Loss: 215117.71072323938\n",
      "Epoch 12, Validation Loss: 188463.20162284368\n",
      "Epoch 13, Validation Loss: 181611.8358650676\n",
      "Epoch 14, Validation Loss: 168888.77335172\n",
      "Epoch 15, Validation Loss: 168055.87457948693\n",
      "Epoch 16, Validation Loss: 187669.06634128993\n",
      "Epoch 17, Validation Loss: 172789.44401696642\n",
      "Epoch 18, Validation Loss: 180516.92892599257\n",
      "Epoch 19, Validation Loss: 148604.8519039979\n",
      "Epoch 20, Validation Loss: 150653.9408209041\n",
      "Epoch 21, Validation Loss: 131378.06736827496\n",
      "Epoch 22, Validation Loss: 136909.75076852055\n",
      "Epoch 23, Validation Loss: 172951.25419386002\n",
      "Epoch 24, Validation Loss: 132896.6761859588\n",
      "Epoch 25, Validation Loss: 130161.40630758755\n",
      "Epoch 26, Validation Loss: 116359.30619526585\n",
      "Epoch 27, Validation Loss: 112751.76595082523\n",
      "Epoch 28, Validation Loss: 121335.15329494416\n",
      "Epoch 29, Validation Loss: 142761.69869704742\n",
      "Epoch 30, Validation Loss: 129768.7543020641\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model3 = LSTM3().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model3.parameters(), lr=0.01)\n",
    "epochs = 30\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model3, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model3, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model3_state_dict': model3.state_dict(),\n",
    "            'optimizer3_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model3.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1101863.1034877584\n",
      "Epoch 2, Validation Loss: 712693.7484476453\n",
      "Epoch 3, Validation Loss: 543179.7868751318\n",
      "Epoch 4, Validation Loss: 464433.1982310347\n",
      "Epoch 5, Validation Loss: 399435.876382851\n",
      "Epoch 6, Validation Loss: 365741.14755962125\n",
      "Epoch 7, Validation Loss: 334788.3300091223\n",
      "Epoch 8, Validation Loss: 315646.42653347\n",
      "Epoch 9, Validation Loss: 288764.9326135782\n",
      "Epoch 10, Validation Loss: 296399.6616173645\n",
      "Epoch 11, Validation Loss: 266274.96979788376\n",
      "Epoch 12, Validation Loss: 287832.1242105421\n",
      "Epoch 13, Validation Loss: 243455.32231170352\n",
      "Epoch 14, Validation Loss: 257248.04324930906\n",
      "Epoch 15, Validation Loss: 258442.86567449945\n",
      "Epoch 16, Validation Loss: 246875.86850911024\n",
      "Epoch 17, Validation Loss: 241208.49869868628\n",
      "Epoch 18, Validation Loss: 242634.9527867785\n",
      "Epoch 19, Validation Loss: 243849.4240000589\n",
      "Epoch 20, Validation Loss: 215716.64719060945\n",
      "Epoch 21, Validation Loss: 221713.9525509979\n",
      "Epoch 22, Validation Loss: 221838.91679974398\n",
      "Epoch 23, Validation Loss: 236323.75914224927\n",
      "Epoch 24, Validation Loss: 386485.04631082964\n",
      "Epoch 25, Validation Loss: 1088122.173348178\n",
      "Epoch 26, Validation Loss: 453045.87059442315\n",
      "Epoch 27, Validation Loss: 337506.4944254169\n",
      "Epoch 28, Validation Loss: 293437.54954929755\n",
      "Epoch 29, Validation Loss: 244115.51850831095\n",
      "Epoch 30, Validation Loss: 227101.13448062522\n",
      "Epoch 31, Validation Loss: 204785.02542152654\n",
      "Epoch 32, Validation Loss: 225693.5968540052\n",
      "Epoch 33, Validation Loss: 221268.3001355379\n",
      "Epoch 34, Validation Loss: 268420.4250903752\n",
      "Epoch 35, Validation Loss: 216293.24550082625\n",
      "Epoch 36, Validation Loss: 202890.26277332785\n",
      "Epoch 37, Validation Loss: 199654.65587357924\n",
      "Epoch 38, Validation Loss: 336053.3962765407\n",
      "Epoch 39, Validation Loss: 208462.10218264096\n",
      "Epoch 40, Validation Loss: 191259.1743042649\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model4 = LSTM4().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=0.01)\n",
    "epochs = 40\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model4, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model4, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model4_state_dict': model4.state_dict(),\n",
    "            'optimizer4_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model4.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1074024.910062742\n",
      "Epoch 2, Validation Loss: 647403.5829355681\n",
      "Epoch 3, Validation Loss: 488727.46757832414\n",
      "Epoch 4, Validation Loss: 407384.21034571924\n",
      "Epoch 5, Validation Loss: 348860.911646633\n",
      "Epoch 6, Validation Loss: 310073.0863467725\n",
      "Epoch 7, Validation Loss: 289566.5205223124\n",
      "Epoch 8, Validation Loss: 273472.6567806718\n",
      "Epoch 9, Validation Loss: 256692.97251467567\n",
      "Epoch 10, Validation Loss: 226570.66618887967\n",
      "Epoch 11, Validation Loss: 211734.2238545845\n",
      "Epoch 12, Validation Loss: 205669.66540298532\n",
      "Epoch 13, Validation Loss: 214958.79239944843\n",
      "Epoch 14, Validation Loss: 223333.57828924022\n",
      "Epoch 15, Validation Loss: 190789.84266543612\n",
      "Epoch 16, Validation Loss: 207241.88694232103\n",
      "Epoch 17, Validation Loss: 203183.05157810636\n",
      "Epoch 18, Validation Loss: 188474.7649591254\n",
      "Epoch 19, Validation Loss: 179619.3111251559\n",
      "Epoch 20, Validation Loss: 178687.05000429333\n",
      "Epoch 21, Validation Loss: 184831.1841580043\n",
      "Epoch 22, Validation Loss: 170281.21296648314\n",
      "Epoch 23, Validation Loss: 198584.95528276087\n",
      "Epoch 24, Validation Loss: 159146.61687479343\n",
      "Epoch 25, Validation Loss: 186760.45857714408\n",
      "Epoch 26, Validation Loss: 190179.20257034092\n",
      "Epoch 27, Validation Loss: 187832.25149614405\n",
      "Epoch 28, Validation Loss: 188324.31208131148\n",
      "Epoch 29, Validation Loss: 204284.29221783092\n",
      "Epoch 30, Validation Loss: 186155.9760037683\n",
      "Epoch 31, Validation Loss: 156667.0962568754\n",
      "Epoch 32, Validation Loss: 212298.71865440538\n",
      "Epoch 33, Validation Loss: 221196.16497445107\n",
      "Epoch 34, Validation Loss: 213876.01451977543\n",
      "Epoch 35, Validation Loss: 195191.45390687228\n",
      "Epoch 36, Validation Loss: 173767.51286355016\n",
      "Epoch 37, Validation Loss: 137705.7925829149\n",
      "Epoch 38, Validation Loss: 160849.10911054618\n",
      "Epoch 39, Validation Loss: 165724.3406425793\n",
      "Epoch 40, Validation Loss: 241905.20881354623\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model5 = LSTM5().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model5.parameters(), lr=0.01)\n",
    "epochs = 40\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model5, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model5, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model5_state_dict': model5.state_dict(),\n",
    "            'optimizer5_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model5.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Validation Loss: 1159557.2247891747\n",
      "Epoch 2, Validation Loss: 655307.8731871947\n",
      "Epoch 3, Validation Loss: 476162.1703109415\n",
      "Epoch 4, Validation Loss: 394740.8585001562\n",
      "Epoch 5, Validation Loss: 334483.2309794739\n",
      "Epoch 6, Validation Loss: 301044.08134956384\n",
      "Epoch 7, Validation Loss: 266651.2639557961\n",
      "Epoch 8, Validation Loss: 257547.20915961865\n",
      "Epoch 9, Validation Loss: 243835.77208197117\n",
      "Epoch 10, Validation Loss: 249558.6638125779\n",
      "Epoch 11, Validation Loss: 209833.65018178232\n",
      "Epoch 12, Validation Loss: 196303.51771599392\n",
      "Epoch 13, Validation Loss: 183482.1958622895\n",
      "Epoch 14, Validation Loss: 171815.7128740403\n",
      "Epoch 15, Validation Loss: 177703.8200525885\n",
      "Epoch 16, Validation Loss: 172615.79236469217\n",
      "Epoch 17, Validation Loss: 160447.503295714\n",
      "Epoch 18, Validation Loss: 159414.22163029885\n",
      "Epoch 19, Validation Loss: 163243.117377842\n",
      "Epoch 20, Validation Loss: 149584.90758724182\n",
      "Epoch 21, Validation Loss: 150360.79658584774\n",
      "Epoch 22, Validation Loss: 138085.86878239358\n",
      "Epoch 23, Validation Loss: 130357.44285402958\n",
      "Epoch 24, Validation Loss: 145010.2737539969\n",
      "Epoch 25, Validation Loss: 159746.0224329773\n",
      "Epoch 26, Validation Loss: 138792.22467461322\n",
      "Epoch 27, Validation Loss: 142933.06180246023\n",
      "Epoch 28, Validation Loss: 141830.39314052294\n",
      "Epoch 29, Validation Loss: 122964.71697767575\n",
      "Epoch 30, Validation Loss: 117822.95883853237\n",
      "Epoch 31, Validation Loss: 147530.21277802845\n",
      "Epoch 32, Validation Loss: 130453.66691763896\n",
      "Epoch 33, Validation Loss: 120272.82311328551\n",
      "Epoch 34, Validation Loss: 119016.98443477409\n",
      "Epoch 35, Validation Loss: 170541.9924689509\n",
      "Epoch 36, Validation Loss: 126389.16168100054\n",
      "Epoch 37, Validation Loss: 117673.68080671965\n",
      "Epoch 38, Validation Loss: 128790.82941057248\n",
      "Epoch 39, Validation Loss: 122563.8897286811\n",
      "Epoch 40, Validation Loss: 124684.02987675097\n"
     ]
    }
   ],
   "source": [
    "# Create the model, criterion, and optimizer\n",
    "best_val_loss = float('inf')\n",
    "model6 = LSTM6().to(device)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model6.parameters(), lr=0.01)\n",
    "epochs = 40\n",
    "\n",
    "# Train and evaluate the model\n",
    "for epoch in range(epochs):  # Adjust the number of epochs as needed\n",
    "    train(model6, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model6, test_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}, Validation Loss: {val_loss}\")\n",
    "\n",
    "    # Save the model if the validation loss is the best we've seen so far.\n",
    "    if val_loss < best_val_loss:\n",
    "        torch.save({\n",
    "            'model6_state_dict': model6.state_dict(),\n",
    "            'optimizer6_state_dict': optimizer.state_dict(),\n",
    "        }, 'models for report/model6.pth')\n",
    "        best_val_loss = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = PriceDataset('ETHUSDT', '1m', '2021-03-01', '2023-04-30')\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, collate_fn=sliding_window_fn, shuffle=False, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: LSTM1 | Test Loss (MAPE): 26369654788897.895%\n",
      "Model: LSTM2 | Test Loss (MAPE): 13145809353347.15%\n",
      "Model: LSTM3 | Test Loss (MAPE): 24179470398066.8%\n",
      "Model: LSTM4 | Test Loss (MAPE): 11057046564136.371%\n",
      "Model: LSTM5 | Test Loss (MAPE): 13964177210777.076%\n",
      "Model: LSTM6 | Test Loss (MAPE): 14331585280605.791%\n"
     ]
    }
   ],
   "source": [
    "model_list = [LSTM1(), LSTM2(), LSTM3(), LSTM4(), LSTM5(), LSTM6()]\n",
    "\n",
    "class MAPELoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        return torch.mean(torch.abs((target - output) * 100 / (target + 1e-10)))\n",
    "\n",
    "\n",
    "for i in range(6):\n",
    "    # Move model to device\n",
    "    model = model_list[i].to(device)\n",
    "\n",
    "    # Initialize the criterion\n",
    "    criterion = MAPELoss()  # Using MAPELoss\n",
    "\n",
    "    load_path = 'models for report/'\n",
    "    model_filename = f'model{i+1}.pth'  # Specify the model filename\n",
    "    model_path = os.path.join(load_path, model_filename)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "    checkpoint = torch.load(model_path)\n",
    "    model.load_state_dict(checkpoint[f'model{i+1}_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint[f'optimizer{i+1}_state_dict'])\n",
    "\n",
    "    # Evaluate the model\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data = data.to(device)\n",
    "            outputs = model(data[:, :-10])  # Pass the first 90 data points through the model\n",
    "            loss = criterion(outputs, data[:, -10:])  # Compare the output with the actual last 10 data points\n",
    "            test_loss += loss.item() * data.size(0)\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print(f'Model: {model.__class__.__name__} | Test Loss (MAPE): {test_loss}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
